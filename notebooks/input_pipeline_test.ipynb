{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "import jax\n",
    "from jax.sharding import Mesh\n",
    "\n",
    "from deeprte.input_pipeline import input_pipeline_interface\n",
    "from deeprte.train_lib import utils\n",
    "from deeprte.train_lib.multihost_dataloading import prefetch_to_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class Config:\n",
    "    # Data\n",
    "    dataset_type: str = \"tfds\"\n",
    "    dataset_name: str = \"rte\"\n",
    "    data_dir: str = \"/workspaces/deeprte/data/tfds\"\n",
    "    train_split: str = \"train[80%:]\"\n",
    "    enable_data_shuffling: bool = True\n",
    "    data_shuffle_seed: int = 42\n",
    "    prefetch_to_device: bool = True\n",
    "\n",
    "    # Parallelism\n",
    "    mesh_axes: tuple[str, ...] = (\"data\", \"fsdp\", \"tensor\")\n",
    "    data_partitions: tuple[str, ...] = ((\"data\", \"fsdp\", \"tensor\"),)\n",
    "    # One axis for each parallelism type may hold a placeholder (-1)\n",
    "    # value to auto-shard based on available slices and devices.\n",
    "    # By default, product of the DCN axes should equal number of slices\n",
    "    # and product of the ICI axes should equal number of devices per slice.\n",
    "    # ICI (Inter-Chip Interconnection): A high-speed connection between\n",
    "    # sets of TPU chips, which form the TPU network.\n",
    "    # DCN (Data Center Network): A connection between the TPU networks;\n",
    "    # not as fast as ICI.\n",
    "    # ICI has around 100x the bandwidth of DCN, but it is not a general\n",
    "    # purpose connection, which is why DCN is necessary for scaling to\n",
    "    # extremely large ML models.\n",
    "    dcn_data_parallelism: int = -1\n",
    "    dcn_fsdp_parallelism: int = 1\n",
    "    dcn_tensor_parallelism: int = 1\n",
    "    ici_data_parallelism: int = 1\n",
    "    ici_fsdp_parallelism: int = -1\n",
    "    ici_tensor_parallelism: int = 1\n",
    "\n",
    "    # Train\n",
    "    global_batch_size_to_load: int = 8\n",
    "    global_batch_size_to_train_on: int = global_batch_size_to_load\n",
    "    collocation_sizes: tuple[int] = (128,)\n",
    "    repeat_batch: int = 1\n",
    "    expansion_factor_real_data: int = -1\n",
    "\n",
    "    # Evaluation\n",
    "    eval_interval: int = -1\n",
    "\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_array = utils.create_device_mesh(config)\n",
    "global_mesh = Mesh(device_array, config.mesh_axes)\n",
    "global_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, eval_iter = input_pipeline_interface.create_data_iterator(\n",
    "    config, global_mesh\n",
    ")\n",
    "\n",
    "if config.prefetch_to_device:\n",
    "    train_iter = prefetch_to_device(train_iter, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.tree.map(lambda x: (x.shape, x.sharding), next(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(train_iter)[\"psi_label\"].sharding.spec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
