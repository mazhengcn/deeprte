{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "188675c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and Slice .mat File Data\n",
    "\n",
    "# This notebook performs the following steps:\n",
    "# 1.  Loads a single `.mat` file.\n",
    "# 2.  Shuffles the data for a specific set of keys.\n",
    "# 3.  Selects the first 500 samples from the shuffled data.\n",
    "# 4.  Saves the result to a new `.mat` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b037f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b8facf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Configuration ---\n",
    "\n",
    "# IMPORTANT: Replace this with the actual path to your source .mat file\n",
    "source_file_path = \"/home/zhuyekun/projects/repos/deeprte/data/raw/train/merge/merged_train.mat\"\n",
    "\n",
    "# IMPORTANT: Define the desired output path and filename for the new data\n",
    "output_path_shuffled = \"/home/zhuyekun/projects/repos/deeprte/data/raw/train/merge/shuffled_500_samples.mat\"\n",
    "\n",
    "# Number of random samples to keep\n",
    "num_samples_to_keep = 500\n",
    "\n",
    "# The specific keys whose arrays you want to shuffle and slice\n",
    "keys_to_process = ['scattering_kernel', 'sigma_a', 'sigma_t', 'phi', 'psi_bc', 'psi_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "302b793e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /home/zhuyekun/projects/repos/deeprte/data/raw/train/merge/merged_train.mat\n",
      "Will keep 500 shuffled samples.\n",
      "Will process the following keys: ['scattering_kernel', 'sigma_a', 'sigma_t', 'phi', 'psi_bc', 'psi_label']\n",
      "Loading data from: /home/zhuyekun/projects/repos/deeprte/data/raw/train/merge/merged_train.mat\n",
      "Found 3000 total samples in the file.\n",
      "Shuffling and slicing data for specified keys...\n",
      "Found 3000 total samples in the file.\n",
      "Shuffling and slicing data for specified keys...\n",
      "Saving 500 shuffled samples to: /home/zhuyekun/projects/repos/deeprte/data/raw/train/merge/shuffled_500_samples.mat\n",
      "Saving 500 shuffled samples to: /home/zhuyekun/projects/repos/deeprte/data/raw/train/merge/shuffled_500_samples.mat\n",
      "Processing complete!\n",
      "\n",
      "Verifying new file...\n",
      "Key: 'phi', New Shape: (500, 41, 41)\n",
      "Key: 'psi_bc', New Shape: (500, 164, 12)\n",
      "Key: 'psi_label', New Shape: (500, 41, 41, 24)\n",
      "Key: 'scattering_kernel', New Shape: (500, 24, 24)\n",
      "Key: 'sigma_a', New Shape: (500, 41, 41)\n",
      "Key: 'sigma_t', New Shape: (500, 41, 41)\n",
      "Processing complete!\n",
      "\n",
      "Verifying new file...\n",
      "Key: 'phi', New Shape: (500, 41, 41)\n",
      "Key: 'psi_bc', New Shape: (500, 164, 12)\n",
      "Key: 'psi_label', New Shape: (500, 41, 41, 24)\n",
      "Key: 'scattering_kernel', New Shape: (500, 24, 24)\n",
      "Key: 'sigma_a', New Shape: (500, 41, 41)\n",
      "Key: 'sigma_t', New Shape: (500, 41, 41)\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Processing Logic ---\n",
    "\n",
    "print(f\"Processing file: {source_file_path}\")\n",
    "print(f\"Will keep {num_samples_to_keep} shuffled samples.\")\n",
    "print(f\"Will process the following keys: {keys_to_process}\")\n",
    "\n",
    "try:\n",
    "    # --- Load Data ---\n",
    "    print(f\"Loading data from: {source_file_path}\")\n",
    "    data = scipy.io.loadmat(source_file_path)\n",
    "\n",
    "    # --- Shuffle and Slice ---\n",
    "\n",
    "    # Get the total number of samples from the first key in the list\n",
    "    # (Assumes all arrays to be processed have the same number of samples)\n",
    "    if not keys_to_process or keys_to_process[0] not in data:\n",
    "        raise ValueError(f\"Key '{keys_to_process[0]}' not found in the file. Cannot determine number of samples.\")\n",
    "\n",
    "    num_total_samples = data[keys_to_process[0]].shape[0]\n",
    "    print(f\"Found {num_total_samples} total samples in the file.\")\n",
    "\n",
    "    if num_total_samples < num_samples_to_keep:\n",
    "        print(f\"Warning: Total samples ({num_total_samples}) is less than requested samples ({num_samples_to_keep}). Keeping all samples.\")\n",
    "        num_samples_to_keep = num_total_samples\n",
    "\n",
    "    # Generate a shuffled sequence of indices\n",
    "    shuffled_indices = np.random.permutation(num_total_samples)\n",
    "\n",
    "    # Take the first `num_samples_to_keep` indices from the shuffled sequence\n",
    "    selected_indices = shuffled_indices[:num_samples_to_keep]\n",
    "\n",
    "    # Create a new dictionary for the processed data, copying all original key-value pairs first\n",
    "    processed_data = {k: v for k, v in data.items() if not k.startswith('__')}\n",
    "\n",
    "    # For the specified keys, overwrite the data with the shuffled and sliced array\n",
    "    print(\"Shuffling and slicing data for specified keys...\")\n",
    "    for key in keys_to_process:\n",
    "        if key in processed_data:\n",
    "            original_array = processed_data[key]\n",
    "            # Use the same shuffled indices to slice the array to maintain consistency\n",
    "            processed_data[key] = original_array[selected_indices]\n",
    "        else:\n",
    "            print(f\"Warning: Key '{key}' not found in the file. Skipping.\")\n",
    "\n",
    "    # --- 3. Save the new file ---\n",
    "    print(f\"Saving {num_samples_to_keep} shuffled samples to: {output_path_shuffled}\")\n",
    "    output_dir = os.path.dirname(output_path_shuffled)\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    scipy.io.savemat(output_path_shuffled, processed_data)\n",
    "    print(\"Processing complete!\")\n",
    "\n",
    "    # --- 4. Verification (Optional) ---\n",
    "    print(\"\\nVerifying new file...\")\n",
    "    verification_data = scipy.io.loadmat(output_path_shuffled)\n",
    "    for key, value in verification_data.items():\n",
    "        if not key.startswith('__'):\n",
    "            # Only print the shape for the keys we processed to confirm the slicing\n",
    "            if key in keys_to_process:\n",
    "                 print(f\"Key: '{key}', New Shape: {value.shape}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Source file not found at {source_file_path}\")\n",
    "except ValueError as ve:\n",
    "    print(f\"Error: {ve}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1a4384",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeprte",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
