{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from deeprte.utils import flat_dict_to_rte_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = \"/workspaces/deeprte/data/train/scattering-kernel/train-scattering-kernel.npz\"\n",
    "\n",
    "np_data = np.load(DATA_FILE)\n",
    "rte_data = flat_dict_to_rte_data(np_data)\n",
    "data, grid = rte_data[\"data\"], rte_data[\"grid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeprte.model.geometry.phase_space import PhaseSpace\n",
    "r, v = grid[\"r\"], grid[\"v\"]\n",
    "r = r.reshape(-1, r.shape[-1])\n",
    "\n",
    "rv = PhaseSpace(\n",
    "    position_coords=r,\n",
    "    velocity_coords=v,\n",
    "    position_weights=0.0,\n",
    "    velocity_weights=0.0,\n",
    ").single_state(cartesian_product=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40344, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rv = rv.reshape(-1, rv.shape[-1])\n",
    "rv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_star = grid[\"v\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeprte.model.utils import cartesian_product\n",
    "import tensorflow as tf\n",
    "# rvv_star = np.concatenate((rv[:, None] + 0.0 * v_star, v_star + 0.0 * rv[:, None]), axis=-1)\n",
    "vv_star = PhaseSpace( \n",
    "        position_coords=rv[...,2:],\n",
    "        velocity_coords=v_star,\n",
    "        position_weights=0.0,\n",
    "        velocity_weights=0.0,\n",
    "        ).single_state(cartesian_product=True)\n",
    "\n",
    "rv_star = PhaseSpace( \n",
    "        position_coords=rv[...,:2],\n",
    "        velocity_coords=v_star,\n",
    "        position_weights=0.0,\n",
    "        velocity_weights=0.0,\n",
    "        ).single_state(cartesian_product=True)\n",
    "\n",
    "vrv_star = tf.concat([vv_star[...,:2],rv_star], axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([40344, 24, 6])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vrv_star.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [[0.  0.1]\n",
      " [0.2 0.3]\n",
      " [0.4 0.5]] b: [[0. 1.]\n",
      " [2. 3.]\n",
      " [4. 5.]\n",
      " [6. 7.]]\n",
      "[[ 0.35052675  1.1914964   0.11277622 -1.5787144 ]\n",
      " [ 0.45674074 -1.39590782  0.33406868  0.33080557]\n",
      " [ 1.20328675 -0.46397825  0.36197406  1.44748718]\n",
      " [-2.36671434  0.88902775  0.4285667   0.01207751]]\n"
     ]
    }
   ],
   "source": [
    "N_x = 3\n",
    "N_v = 4\n",
    "\n",
    "a = np.linspace(0,0.5,num = 2*N_x).reshape(N_x,2)\n",
    "b = np.linspace(0,7,num = 2*N_v).reshape(N_v,2)\n",
    "print(\"a:\",a,\"b:\",b)\n",
    "\n",
    "P = np.random.randn(N_v, N_v)\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = PhaseSpace( \n",
    "        position_coords=a,\n",
    "        velocity_coords=b,\n",
    "        position_weights=0.0,\n",
    "        velocity_weights=0.0,\n",
    "        ).single_state(cartesian_product=True)\n",
    "r = r.reshape(-1, r.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = tf.concat([r, tf.tile(P, multiples=[N_x,1])],axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(12, 8), dtype=float64, numpy=\n",
       "array([[ 0.        ,  0.1       ,  0.        ,  1.        ,  0.35052675,\n",
       "         1.1914964 ,  0.11277622, -1.5787144 ],\n",
       "       [ 0.        ,  0.1       ,  2.        ,  3.        ,  0.45674074,\n",
       "        -1.39590782,  0.33406868,  0.33080557],\n",
       "       [ 0.        ,  0.1       ,  4.        ,  5.        ,  1.20328675,\n",
       "        -0.46397825,  0.36197406,  1.44748718],\n",
       "       [ 0.        ,  0.1       ,  6.        ,  7.        , -2.36671434,\n",
       "         0.88902775,  0.4285667 ,  0.01207751],\n",
       "       [ 0.2       ,  0.3       ,  0.        ,  1.        ,  0.35052675,\n",
       "         1.1914964 ,  0.11277622, -1.5787144 ],\n",
       "       [ 0.2       ,  0.3       ,  2.        ,  3.        ,  0.45674074,\n",
       "        -1.39590782,  0.33406868,  0.33080557],\n",
       "       [ 0.2       ,  0.3       ,  4.        ,  5.        ,  1.20328675,\n",
       "        -0.46397825,  0.36197406,  1.44748718],\n",
       "       [ 0.2       ,  0.3       ,  6.        ,  7.        , -2.36671434,\n",
       "         0.88902775,  0.4285667 ,  0.01207751],\n",
       "       [ 0.4       ,  0.5       ,  0.        ,  1.        ,  0.35052675,\n",
       "         1.1914964 ,  0.11277622, -1.5787144 ],\n",
       "       [ 0.4       ,  0.5       ,  2.        ,  3.        ,  0.45674074,\n",
       "        -1.39590782,  0.33406868,  0.33080557],\n",
       "       [ 0.4       ,  0.5       ,  4.        ,  5.        ,  1.20328675,\n",
       "        -0.46397825,  0.36197406,  1.44748718],\n",
       "       [ 0.4       ,  0.5       ,  6.        ,  7.        , -2.36671434,\n",
       "         0.88902775,  0.4285667 ,  0.01207751]])>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv_prime = PhaseSpace( \n",
    "        position_coords=grid[\"v\"],\n",
    "        velocity_coords= grid[\"r\"][:,1,:],\n",
    "        position_weights=0.0,\n",
    "        velocity_weights=0.0,\n",
    "        ).single_state(cartesian_product=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 41, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vv_prime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': (TensorSpec(shape=(41, 4), dtype=tf.float64, name=None),\n",
       "  TensorSpec(shape=(41, 4), dtype=tf.float64, name=None),\n",
       "  FunctionInputs(x=TensorSpec(shape=(24, 41, 4), dtype=tf.float64, name=None), f=TensorSpec(shape=(24, 41, 4), dtype=tf.float64, name=None)))}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deeprte.model.modules import FunctionInputs\n",
    "import tensorflow as tf\n",
    "dict = {\"inputs\":(np.random.randn(*vv_prime.shape[1:]),np.random.randn(*vv_prime.shape[1:]),FunctionInputs(x = vv_prime, f = np.random.randn(*vv_prime.shape)))}\n",
    "\n",
    "ds = tf.data.Dataset.from_tensors(dict)\n",
    "ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_data(data):\n",
    "    batch = tf.nest.map_structure(lambda x: tf.gather(x, 1), data[\"inputs\"][:1])\n",
    "    # batch = tf.gather(data[\"inputs\"][2], -2,)\n",
    "    return batch\n",
    "dataset = ds.map(gather_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSpec(shape=(24, 41, 4), dtype=tf.float64, name=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_batch(i, data):\n",
    "    batch = dict(**data)\n",
    "    rv_r, rv_v = tf.nest.map_structure(\n",
    "        lambda x: tf.gather(x, i, axis=-2), data[\"inputs\"][:2]\n",
    "    )\n",
    "    batch[\"inputs\"] = (rv_r, rv_v) + data[\"inputs\"][2:]\n",
    "    batch[\"labels\"] = tf.gather(data[\"labels\"], i, axis=-1)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import enum\n",
    "import pathlib\n",
    "from collections.abc import Generator, Mapping, Sequence\n",
    "\n",
    "import jax\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from absl import logging\n",
    "\n",
    "from deeprte.model.geometry.phase_space import PhaseSpace\n",
    "from deeprte.model.modules import FunctionInputs\n",
    "from deeprte.utils import flat_dict_to_rte_data\n",
    "\n",
    "Batch = Mapping[str, np.ndarray]\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "\n",
    "def log_shapes(d: dict, name: str):\n",
    "    logs = f\"{name} shapes\"\n",
    "    for k, v in get_nest_dict_shape(d).items():\n",
    "        logs += f\", {k:s}: {v}\"\n",
    "\n",
    "    logging.info(logs)\n",
    "\n",
    "\n",
    "def get_nest_dict_shape(d):\n",
    "    return tf.nest.map_structure(lambda x: x.shape, d)\n",
    "\n",
    "\n",
    "class Split(enum.Enum):\n",
    "    \"\"\"Datset split.\"\"\"\n",
    "\n",
    "    TRAIN = 1\n",
    "    TRAIN_AND_VALID = 2\n",
    "    VALID = 3\n",
    "    TEST = 4\n",
    "\n",
    "    @classmethod\n",
    "    def from_string(cls, name: str) -> \"Split\":\n",
    "        return {\n",
    "            \"TRAIN\": Split.TRAIN,\n",
    "            \"TRAIN_AND_VALID\": Split.TRAIN_AND_VALID,\n",
    "            \"VALID\": Split.VALID,\n",
    "            \"VALIDATION\": Split.VALID,\n",
    "            \"TEST\": Split.TEST,\n",
    "        }[name.upper()]\n",
    "\n",
    "    @property\n",
    "    def num_examples(self):\n",
    "        return {\n",
    "            Split.TRAIN: 1600,\n",
    "            Split.TRAIN_AND_VALID: 1600,\n",
    "            Split.VALID: 400,\n",
    "            Split.TEST: 400,\n",
    "        }[self]\n",
    "\n",
    "\n",
    "def load(\n",
    "    data_path: str | pathlib.Path,\n",
    "    split: Split,\n",
    "    is_training: bool,\n",
    "    # batch_sizes should be:\n",
    "    # [device_count, per_device_outer_batch_size]\n",
    "    # total_batch_size = device_count * per_device_outer_batch_size\n",
    "    batch_sizes: Sequence[int],\n",
    "    # collocation_sizes should be:\n",
    "    # total_collocation_size or\n",
    "    # [residual_size, boundary_size, quadrature_size]\n",
    "    collocation_sizes: int | Sequence[int] | None,\n",
    "    # repeat number of inner batch, for training the same batch with\n",
    "    # {repeat} steps of different collocation points\n",
    "    repeat: int | None = 1,\n",
    "    # shuffle buffer size\n",
    "    buffer_size: int = 5_000,\n",
    "    # Dataset options\n",
    "    threadpool_size: int = 48,\n",
    "    max_intra_op_parallelism: int = 1,\n",
    ") -> Generator[Batch, None, None]:\n",
    "\n",
    "    if is_training:\n",
    "        if not collocation_sizes and not repeat:\n",
    "            raise ValueError(\n",
    "                \"`collocation_sizes` and `repeat` should not be None\"\n",
    "                \"when `is_training=True`\"\n",
    "            )\n",
    "\n",
    "    start, end = _shard(split, jax.process_index(), jax.process_count())\n",
    "\n",
    "    (ds, (grid, total_grid_sizes)), _ = _load_and_split_dataset(\n",
    "        data_path, split, from_=start, end=end\n",
    "    )\n",
    "\n",
    "    options = tf.data.Options()\n",
    "    options.threading.max_intra_op_parallelism = max_intra_op_parallelism\n",
    "    options.threading.private_threadpool_size = threadpool_size\n",
    "    options.experimental_optimization.map_parallelization = True\n",
    "    options.experimental_optimization.parallel_batch = True\n",
    "    if is_training:\n",
    "        options.deterministic = False\n",
    "\n",
    "    if is_training:\n",
    "        if jax.process_count() > 1:\n",
    "            # Only cache if we are reading a subset of the dataset.\n",
    "            ds = ds.cache()\n",
    "        ds = ds.repeat()\n",
    "        ds = ds.shuffle(buffer_size=buffer_size)\n",
    "        ds = _repeat_batch(batch_sizes, ds, repeat)\n",
    "\n",
    "    # batch per_device outer first,\n",
    "    # since they share the same random grid points\n",
    "    ds = ds.batch(batch_sizes[-1], drop_remainder=True)\n",
    "    # construct the inputs structure\n",
    "    ds = process_inputs(ds, grid)\n",
    "    # batch device dim\n",
    "    ds = ds.batch(batch_sizes[0], drop_remainder=True)\n",
    "\n",
    "    if is_training:\n",
    "        ds = sample_from_dataset(ds, collocation_sizes, total_grid_sizes)\n",
    "\n",
    "    ds = ds.prefetch(AUTOTUNE)\n",
    "    ds = ds.with_options(options)\n",
    "\n",
    "    # convert to a numpy generator\n",
    "    yield from tfds.as_numpy(ds)\n",
    "\n",
    "\n",
    "def sample_from_dataset(\n",
    "    dataset: tf.data.Dataset,\n",
    "    collocation_sizes: int | Sequence[int],\n",
    "    total_grid_sizes: int | Sequence[int],\n",
    "    sampler: str = \"uniform\",\n",
    "    seed: int = jax.process_index(),\n",
    "):\n",
    "\n",
    "    g = tf.random.Generator.from_seed(seed)\n",
    "\n",
    "    if sampler == \"uniform\":\n",
    "\n",
    "        def _sample_fn(_):\n",
    "            idx = g.uniform(\n",
    "                (collocation_sizes,),\n",
    "                minval=0,\n",
    "                maxval=total_grid_sizes,\n",
    "                dtype=tf.int64,\n",
    "            )\n",
    "            return idx\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Sample from {sampler} distribution is not implemented.\")\n",
    "\n",
    "    # generate random sample indices\n",
    "    indices_ds = tf.data.Dataset.range(1).repeat()\n",
    "    indices_ds = indices_ds.map(_sample_fn, num_parallel_calls=AUTOTUNE)\n",
    "    ds = slice_inputs(indices_ds, dataset)\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def process_inputs(data: tf.data.Dataset, grid: Mapping[str, np.ndarray]):\n",
    "\n",
    "    ds = tf.data.Dataset.zip((data, tf.data.Dataset.from_tensors(grid).repeat()))\n",
    "\n",
    "    def _construct_batch(data, grid):\n",
    "\n",
    "        sigma = tf.stack([data[\"sigma_t\"], data[\"sigma_a\"]], axis=-1)\n",
    "        psi_bc = data[\"psi_bc\"]\n",
    "        psi_label = data[\"psi_label\"]\n",
    "\n",
    "        r, rv = grid[\"r\"], grid[\"rv\"]\n",
    "        rv_prime, w_prime = grid[\"rv_prime\"], grid[\"w_prime\"]\n",
    "        rv_r, rv_v = tf.split(rv, num_or_size_splits=2, axis=-1)\n",
    "\n",
    "        # w_star = grid[\"w_star\"]\n",
    "        w_star = grid[\"w_angle\"]\n",
    "        # scattering_kernel = grid[\"scattering_kernel\"]\n",
    "        scattering_kernel = np.random.randn(\n",
    "            (\n",
    "                rv_v.shape[0],\n",
    "                w_star.shape[0],\n",
    "            )\n",
    "        )\n",
    "\n",
    "        vv_star = grid[\"vv_star\"]\n",
    "\n",
    "        return {\n",
    "            \"inputs\": (\n",
    "                rv_r,\n",
    "                rv_v,\n",
    "                FunctionInputs(x=r, f=sigma),\n",
    "                FunctionInputs(x=rv_prime, f=psi_bc * w_prime),\n",
    "                FunctionInputs(\n",
    "                    x=vv_star,\n",
    "                    f=(scattering_kernel - 1) * w_star,\n",
    "                ),\n",
    "            ),\n",
    "            \"labels\": psi_label,\n",
    "        }\n",
    "\n",
    "    ds = ds.map(_construct_batch, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def slice_inputs(indices_dataset: tf.data.Dataset, inputs: tf.data.Dataset):\n",
    "\n",
    "    dataset = tf.data.Dataset.zip((indices_dataset, inputs))\n",
    "\n",
    "    def grab_batch(i, data):\n",
    "        batch = dict(**data)\n",
    "        rv_r, rv_v = tf.nest.map_structure(\n",
    "            lambda x: tf.gather(x, i, axis=-2), data[\"inputs\"][:2]\n",
    "        )\n",
    "        scattering_kernel = tf.nest.map_structure(\n",
    "            lambda x: tf.gather(x, i, axis=-2), data[\"inputs\"][3:4]\n",
    "        )\n",
    "        batch[\"inputs\"] = (rv_r, rv_v) + data[\"inputs\"][2:4] + scattering_kernel\n",
    "        batch[\"labels\"] = tf.gather(data[\"labels\"], i, axis=-1)\n",
    "        return batch\n",
    "\n",
    "    dataset = dataset.map(grab_batch, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def _repeat_batch(\n",
    "    batch_sizes: int | Sequence[int],\n",
    "    ds: tf.data.Dataset,\n",
    "    repeat: int = 1,\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"Tiles the inner most batch dimension.\"\"\"\n",
    "    if repeat <= 1:\n",
    "        return ds\n",
    "    # Perform regular batching with reduced number of elements.\n",
    "    for batch_size in reversed(batch_sizes):\n",
    "        ds = ds.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    # Repeat batch.\n",
    "    fn = lambda x: tf.tile(  # noqa: E731\n",
    "        x, multiples=[repeat] + [1] * (len(x.shape) - 1)\n",
    "    )\n",
    "\n",
    "    def repeat_inner_batch(example):\n",
    "        return tf.nest.map_structure(fn, example)\n",
    "\n",
    "    ds = ds.map(repeat_inner_batch, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    # Unbatch.\n",
    "    for _ in batch_sizes:\n",
    "        ds = ds.unbatch()\n",
    "    return ds\n",
    "\n",
    "\n",
    "def _load_and_split_dataset(\n",
    "    path_npz: str | pathlib.Path,\n",
    "    split: Split,\n",
    "    end: int,\n",
    "    from_: int = 0,\n",
    "    pre_shuffle: bool = True,\n",
    "    seed: int = 0,\n",
    ") -> tuple[tuple[tf.data.Dataset, Mapping[str, np.ndarray]], Split]:\n",
    "\n",
    "    if not isinstance(path_npz, pathlib.Path):\n",
    "        path_npz = pathlib.Path(path_npz)\n",
    "\n",
    "    with tf.io.gfile.GFile(path_npz, \"rb\") as fp:\n",
    "        npzfile = np.load(fp, allow_pickle=False)\n",
    "        rte_data = flat_dict_to_rte_data(npzfile)\n",
    "        data, grid = rte_data[\"data\"], rte_data[\"grid\"]\n",
    "\n",
    "    if pre_shuffle:\n",
    "        rng = np.random.default_rng(seed=seed)\n",
    "        shuffled_indices = rng.permutation(data[\"psi_label\"].shape[0])\n",
    "        data = tf.nest.map_structure(\n",
    "            lambda x: np.take(x, shuffled_indices, axis=0), data\n",
    "        )\n",
    "\n",
    "    def _flatten_fn(example):\n",
    "        return tf.nest.map_structure(lambda x: tf.reshape(x, [-1]), example)\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices(\n",
    "        tf.nest.map_structure(lambda arr: arr[from_:end], data)\n",
    "    ).map(_flatten_fn, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    grid = preprocess_grid(grid)\n",
    "\n",
    "    return (ds, grid), split\n",
    "\n",
    "\n",
    "def _shard(split: Split, shard_index: int, num_shards: int) -> tuple[int, int]:\n",
    "    \"\"\"Returns [start, end) for the given shard index.\"\"\"\n",
    "    assert shard_index < num_shards\n",
    "    arange = np.arange(split.num_examples)\n",
    "    shard_range = np.array_split(arange, num_shards)[shard_index]\n",
    "    start, end = shard_range[0], (shard_range[-1] + 1)\n",
    "    if split == Split.TRAIN_AND_VALID:\n",
    "        offset = Split.TEST.num_examples\n",
    "        start += offset\n",
    "        end += offset\n",
    "    return start, end\n",
    "\n",
    "\n",
    "def preprocess_grid(\n",
    "    grid: Mapping[str, np.ndarray], is_training: bool = True\n",
    ") -> Mapping[str, np.ndarray]:\n",
    "    r, v = grid[\"r\"], grid[\"v\"]\n",
    "    r = r.reshape(-1, r.shape[-1])\n",
    "    grid[\"r\"] = r\n",
    "\n",
    "    # rv = np.concatenate((r[:, None] + 0.0 * v, v + 0.0 * r[:, None]), axis=-1)\n",
    "    rv = PhaseSpace(\n",
    "        position_coords=r,\n",
    "        velocity_coords=v,\n",
    "        position_weights=0.0,\n",
    "        velocity_weights=0.0,\n",
    "    ).single_state(cartesian_product=True)\n",
    "    rv = rv.reshape(-1, rv.shape[-1])\n",
    "    grid[\"rv\"] = rv\n",
    "    total_grid_size = rv.shape[0]\n",
    "\n",
    "    rv_prime, w_prime = grid[\"rv_prime\"], grid[\"w_prime\"]\n",
    "    grid[\"rv_prime\"] = rv_prime.reshape(-1, rv_prime.shape[-1])\n",
    "    grid[\"w_prime\"] = w_prime.flatten()\n",
    "\n",
    "    # v_star = grid[\"v_star\"]\n",
    "    v_star = grid[\"v\"]\n",
    "    _, rv_v = tf.split(rv, num_or_size_splits=2, axis=-1)\n",
    "\n",
    "    vv_star = PhaseSpace(\n",
    "        position_coords=rv_v,\n",
    "        velocity_coords=v_star,\n",
    "        position_weights=0.0,\n",
    "        velocity_weights=0.0,\n",
    "    ).single_state(cartesian_product=True)\n",
    "\n",
    "    grid[\"vv_star\"] = vv_star\n",
    "\n",
    "    if is_training:\n",
    "        del grid[\"w_angle\"], grid[\"v\"]\n",
    "\n",
    "    return grid, total_grid_size\n",
    "\n",
    "\n",
    "def unstack_np_array(arr, axis=0):\n",
    "    return list(np.swapaxes(arr, axis, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load(\"/workspaces/deeprte/data/train/scattering-kernel/train-scattering-kernel.npz\",split=Split.TRAIN_AND_VALID,is_training=True, batch_sizes=[jax.local_device_count(), 1],collocation_sizes=1,repeat=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret '<tf.Tensor: shape=(40344, 2), dtype=float64, numpy=\narray([[ 0.25544139,  0.25544139],\n       [ 0.69309572,  0.28708965],\n       [ 0.28708965,  0.69309572],\n       ...,\n       [ 0.2513426 , -0.93802334],\n       [ 0.68668074, -0.68668074],\n       [ 0.93802334, -0.2513426 ]])>' as a data type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m ex \u001b[39min\u001b[39;00m ds:\n\u001b[1;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(ex)\n",
      "Cell \u001b[0;32mIn [16], line 92\u001b[0m, in \u001b[0;36mload\u001b[0;34m(data_path, split, is_training, batch_sizes, collocation_sizes, repeat, buffer_size, threadpool_size, max_intra_op_parallelism)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     86\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`collocation_sizes` and `repeat` should not be None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     87\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mwhen `is_training=True`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m         )\n\u001b[1;32m     90\u001b[0m start, end \u001b[39m=\u001b[39m _shard(split, jax\u001b[39m.\u001b[39mprocess_index(), jax\u001b[39m.\u001b[39mprocess_count())\n\u001b[0;32m---> 92\u001b[0m (ds, (grid, total_grid_sizes)), _ \u001b[39m=\u001b[39m _load_and_split_dataset(\n\u001b[1;32m     93\u001b[0m     data_path, split, from_\u001b[39m=\u001b[39;49mstart, end\u001b[39m=\u001b[39;49mend\n\u001b[1;32m     94\u001b[0m )\n\u001b[1;32m     96\u001b[0m options \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mOptions()\n\u001b[1;32m     97\u001b[0m options\u001b[39m.\u001b[39mthreading\u001b[39m.\u001b[39mmax_intra_op_parallelism \u001b[39m=\u001b[39m max_intra_op_parallelism\n",
      "Cell \u001b[0;32mIn [16], line 286\u001b[0m, in \u001b[0;36m_load_and_split_dataset\u001b[0;34m(path_npz, split, end, from_, pre_shuffle, seed)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mmap_structure(\u001b[39mlambda\u001b[39;00m x: tf\u001b[39m.\u001b[39mreshape(x, [\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]), example)\n\u001b[1;32m    282\u001b[0m ds \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensor_slices(\n\u001b[1;32m    283\u001b[0m     tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mmap_structure(\u001b[39mlambda\u001b[39;00m arr: arr[from_:end], data)\n\u001b[1;32m    284\u001b[0m )\u001b[39m.\u001b[39mmap(_flatten_fn, num_parallel_calls\u001b[39m=\u001b[39mAUTOTUNE)\n\u001b[0;32m--> 286\u001b[0m grid \u001b[39m=\u001b[39m preprocess_grid(grid)\n\u001b[1;32m    288\u001b[0m \u001b[39mreturn\u001b[39;00m (ds, grid), split\n",
      "Cell \u001b[0;32mIn [16], line 330\u001b[0m, in \u001b[0;36mpreprocess_grid\u001b[0;34m(grid, is_training)\u001b[0m\n\u001b[1;32m    327\u001b[0m v_star \u001b[39m=\u001b[39m grid[\u001b[39m\"\u001b[39m\u001b[39mv\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    328\u001b[0m _, rv_v \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39msplit(rv, num_or_size_splits\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 330\u001b[0m vv_star \u001b[39m=\u001b[39m PhaseSpace(\n\u001b[1;32m    331\u001b[0m     position_coords\u001b[39m=\u001b[39;49mrv_v,\n\u001b[1;32m    332\u001b[0m     velocity_coords\u001b[39m=\u001b[39;49mv_star,\n\u001b[1;32m    333\u001b[0m     position_weights\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m,\n\u001b[1;32m    334\u001b[0m     velocity_weights\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m,\n\u001b[1;32m    335\u001b[0m )\u001b[39m.\u001b[39;49msingle_state(cartesian_product\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    337\u001b[0m grid[\u001b[39m\"\u001b[39m\u001b[39mvv_star\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m vv_star\n\u001b[1;32m    339\u001b[0m \u001b[39mif\u001b[39;00m is_training:\n",
      "File \u001b[0;32m/workspaces/deeprte/deeprte/model/geometry/phase_space.py:59\u001b[0m, in \u001b[0;36mPhaseSpace.single_state\u001b[0;34m(self, cartesian_product)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mif\u001b[39;00m cartesian_product:\n\u001b[1;32m     58\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mr\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m _cartesian_product(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mr, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mv)\n\u001b[1;32m     60\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mr\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m/workspaces/deeprte/deeprte/model/geometry/phase_space.py:37\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(*arrs)\u001b[0m\n\u001b[1;32m     25\u001b[0m Float \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m np\u001b[39m.\u001b[39mfloat32\n\u001b[1;32m     28\u001b[0m _concat \u001b[39m=\u001b[39m (\n\u001b[1;32m     29\u001b[0m     \u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39marrs: jnp\u001b[39m.\u001b[39mconcatenate(arrs, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     30\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(arrs[\u001b[39m0\u001b[39m], jnp\u001b[39m.\u001b[39mndarray)\n\u001b[1;32m     31\u001b[0m     \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39mconcatenate(arrs, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     34\u001b[0m _cartesian_product \u001b[39m=\u001b[39m (\n\u001b[1;32m     35\u001b[0m     \u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39marrs: jax_cartesian_product(\u001b[39m*\u001b[39marrs)\n\u001b[1;32m     36\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(arrs[\u001b[39m0\u001b[39m], jnp\u001b[39m.\u001b[39mndarray)\n\u001b[0;32m---> 37\u001b[0m     \u001b[39melse\u001b[39;00m cartesian_product(\u001b[39m*\u001b[39;49marrs)\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     41\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mPhaseSpace\u001b[39;00m(NamedTuple):\n\u001b[1;32m     42\u001b[0m     \u001b[39m\"\"\"Hold a pair of position and velocity coordinates in the phase space.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39m    Here we allow position and velocity arrays have different first dimension,\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39m    which by a cartesian product to generate a single state.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \n\u001b[1;32m     46\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/deeprte/deeprte/model/utils.py:37\u001b[0m, in \u001b[0;36mcartesian_product\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m     34\u001b[0m ls \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mmap\u001b[39m(\u001b[39mlen\u001b[39m, arrays)]\n\u001b[1;32m     35\u001b[0m inds \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mmap\u001b[39m(np\u001b[39m.\u001b[39marange, ls)]\n\u001b[0;32m---> 37\u001b[0m dtype \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mresult_type(\u001b[39m*\u001b[39;49marrays)\n\u001b[1;32m     38\u001b[0m arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty(ls \u001b[39m+\u001b[39m [la \u001b[39m*\u001b[39m d], dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m     40\u001b[0m \u001b[39mfor\u001b[39;00m i, ind \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(np\u001b[39m.\u001b[39mix_(\u001b[39m*\u001b[39minds)):\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mresult_type\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret '<tf.Tensor: shape=(40344, 2), dtype=float64, numpy=\narray([[ 0.25544139,  0.25544139],\n       [ 0.69309572,  0.28708965],\n       [ 0.28708965,  0.69309572],\n       ...,\n       [ 0.2513426 , -0.93802334],\n       [ 0.68668074, -0.68668074],\n       [ 0.93802334, -0.2513426 ]])>' as a data type"
     ]
    }
   ],
   "source": [
    "for ex in ds:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
